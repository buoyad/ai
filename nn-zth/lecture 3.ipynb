{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea1b62af-47c4-4e6c-b705-a95c7cbf6ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c25c31bd-abef-430d-a8c0-c7105553b10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('data/names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2c908c3-2f1e-4f48-8d32-5a00dfd11675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80a85e8a-8c6c-44b4-bad4-a3e8df2dd875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and map to integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "efa3105a-b009-4c0e-b5ee-88a8e33887e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "\n",
    "block_size = 3 # context length, how many characters to predict the next one\n",
    "X, Y = [], []\n",
    "for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "X = torch.tensor(X) # input dataset, previous 3 characters of context, padded with '.'\n",
    "Y = torch.tensor(Y) # labels, next character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d402bbcf-3bcf-491f-bbd4-64ef8e9f13ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.int64, torch.Size([228146]), torch.int64)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ead5b5d-3247-4922-a1b4-b57e9ddf1d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((27, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d09ba55e-032d-446f-8e97-9cddc350c6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2067, -0.9111])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this method is more in line with considering the embedding table\n",
    "# the \"first layer\" of the neural network, converting our inputs\n",
    "# to one-hot and then indexing into the embedding that way.\n",
    "\n",
    "# however, it's much faster to just directly index, so instead we will\n",
    "# manually pre-index into the embedding table and then consider the\n",
    "# embedded raw data as the input to our neural net.\n",
    "\n",
    "F.one_hot(torch.tensor(5), num_classes=27).float() @ C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7cfa0e4e-7f47-435f-8b83-5c66aa7d896f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2067, -0.9111])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e458d753-9267-4921-990d-1387e5950b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can fetch embeddings for every character in X just by directly indexing\n",
    "C[X].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6b51ba4b-74e3-457f-b5bb-59ac22d98778",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = C[X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c4909d98-a662-4be2-98f9-f44cb6c1ac6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn((2 * block_size, 100)) # 2 inputs per character of context\n",
    "b1 = torch.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "61b872b4-c400-4234-b22d-4f2fc37b0f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 2]), torch.Size([6, 100]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# incompatible sizes, we need to concatenate the 3,2 portions of our embeddings\n",
    "emb.shape, W1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4b789932-e657-42db-8639-9d2fa3ce23a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]], 1).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fcfebc7b-f1ef-440c-abf6-aa86c902f4c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or, to do WLOG w/r/t block_size\n",
    "torch.cat(torch.unbind(emb, 1), 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "10c2077f-d692-4097-bdda-c233969d8639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(18)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c81f5df8-361b-4a75-80d5-1e0b8edae720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "09d313dd-2732-4643-a330-4db180c1d193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8],\n",
       "        [ 9, 10, 11, 12, 13, 14, 15, 16, 17]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view is extremely efficient in pytorch\n",
    "# the underlying storage is not affected at all, only organizational info about\n",
    "# how the storage is represented to python.\n",
    "# storage offsets, strides, and shapes are changed so that identical \n",
    "# sequences of bytes can represent different shapes of data.\n",
    "# http://blog.ezyang.com/2019/05/pytorch-internals/\n",
    "a.view(2, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "96c1d59f-c55e-45b8-b7b6-52e5764c97d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fa5d7859-b8f4-47d5-aa49-c5b26347f689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.1152, -0.3656,  1.1152, -0.3656,  1.1152, -0.3656],\n",
       "         [ 1.1152, -0.3656,  1.1152, -0.3656, -0.2067, -0.9111],\n",
       "         [ 1.1152, -0.3656, -0.2067, -0.9111,  0.1083,  1.6349],\n",
       "         [-0.2067, -0.9111,  0.1083,  1.6349,  0.1083,  1.6349],\n",
       "         [ 0.1083,  1.6349,  0.1083,  1.6349,  0.8300, -0.3442],\n",
       "         [ 1.1152, -0.3656,  1.1152, -0.3656,  1.1152, -0.3656],\n",
       "         [ 1.1152, -0.3656,  1.1152, -0.3656, -0.5170,  0.0476],\n",
       "         [ 1.1152, -0.3656, -0.5170,  0.0476,  1.1058, -0.1189],\n",
       "         [-0.5170,  0.0476,  1.1058, -0.1189,  0.8579,  1.3922],\n",
       "         [ 1.1058, -0.1189,  0.8579,  1.3922, -0.5378, -0.6277],\n",
       "         [ 0.8579,  1.3922, -0.5378, -0.6277,  0.8579,  1.3922],\n",
       "         [-0.5378, -0.6277,  0.8579,  1.3922,  0.8300, -0.3442],\n",
       "         [ 1.1152, -0.3656,  1.1152, -0.3656,  1.1152, -0.3656],\n",
       "         [ 1.1152, -0.3656,  1.1152, -0.3656,  0.8300, -0.3442],\n",
       "         [ 1.1152, -0.3656,  0.8300, -0.3442, -0.5378, -0.6277],\n",
       "         [ 0.8300, -0.3442, -0.5378, -0.6277,  0.8300, -0.3442],\n",
       "         [ 1.1152, -0.3656,  1.1152, -0.3656,  1.1152, -0.3656],\n",
       "         [ 1.1152, -0.3656,  1.1152, -0.3656,  0.8579,  1.3922],\n",
       "         [ 1.1152, -0.3656,  0.8579,  1.3922,  2.3202, -0.2073],\n",
       "         [ 0.8579,  1.3922,  2.3202, -0.2073,  0.8300, -0.3442],\n",
       "         [ 2.3202, -0.2073,  0.8300, -0.3442, -0.4431,  0.6857],\n",
       "         [ 0.8300, -0.3442, -0.4431,  0.6857, -0.2067, -0.9111],\n",
       "         [-0.4431,  0.6857, -0.2067, -0.9111,  1.1058, -0.1189],\n",
       "         [-0.2067, -0.9111,  1.1058, -0.1189,  1.1058, -0.1189],\n",
       "         [ 1.1058, -0.1189,  1.1058, -0.1189,  0.8300, -0.3442],\n",
       "         [ 1.1152, -0.3656,  1.1152, -0.3656,  1.1152, -0.3656],\n",
       "         [ 1.1152, -0.3656,  1.1152, -0.3656,  2.3202, -0.2073],\n",
       "         [ 1.1152, -0.3656,  2.3202, -0.2073, -0.5170,  0.0476],\n",
       "         [ 2.3202, -0.2073, -0.5170,  0.0476,  0.1616,  0.6294],\n",
       "         [-0.5170,  0.0476,  0.1616,  0.6294, -0.5207,  0.4617],\n",
       "         [ 0.1616,  0.6294, -0.5207,  0.4617,  0.8579,  1.3922],\n",
       "         [-0.5207,  0.4617,  0.8579,  1.3922,  0.8300, -0.3442]]),\n",
       " tensor([-0.2067, -0.9111]),\n",
       " tensor([0.1083, 1.6349]),\n",
       " tensor([0.1083, 1.6349]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.view(32, 6), emb[3, 0], emb[3, 1], emb[3, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0bb07285-d8b1-4837-8c73-e6ac84395341",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.tanh(emb.view(-1, block_size * 2) @ W1 + b1) \n",
    "# b1 is broadcasted so the same bias is added to every row of output, which is what we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a67dcb50-3838-43c3-abff-4e1d14e8ecb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "353c2b25-ab8c-4467-969e-ecd926863d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = torch.randn((100, 27))\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ee3bfd1b-2413-4af4-9431-c2023f012f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = h @ W2 + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e895cfe0-00b6-4893-a83d-b9b98f73b744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bf0f1270-4ebf-48bd-b142-f06ddf231611",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ef5e0c9a-e926-4bdd-a7de-6c8575e134be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8849ff24-702d-4815-94af-4e9d67092e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = -prob[torch.arange(32), Y].log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "d20dbc16-b753-4843-b067-6d90a247d17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "emb_size = 2\n",
    "hidden_size = 100\n",
    "C = torch.randn((27, emb_size), generator=g)\n",
    "W1 = torch.randn((emb_size * block_size, hidden_size), generator=g)\n",
    "b1 = torch.randn(hidden_size, generator=g)\n",
    "W2 = torch.randn((hidden_size, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ef401bce-f5a1-4a98-b4d9-21b1fa3fb93b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "426620ef-39e8-4323-a63a-40712686d614",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = C[X] # 32x2x2\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # -1 means torch will infer the size - output is 32x100\n",
    "logits = h @ W2 + b2 # 32x27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "619b1aba-ef5f-4db3-a8a6-9fb2f5d00907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(19.5052)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is much more efficient - avoids all the intermediate allocation\n",
    "# and backward pass is much more efficient without all the intermediate operations\n",
    "loss = F.cross_entropy(logits, Y) # identical!\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1232fbb5-4b17-499e-a5d4-9df999942d8f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., nan])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.tensor([-100, -3, 0, 100])\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum()\n",
    "probs \n",
    "# bad! our custom rolled cross_entropy results in a \n",
    "# nan because we run out of dynamic range on high inputs passed to exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e1e9d1eb-917b-44e3-8f6f-6195f8550d98",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 1.4013e-45, 3.7835e-44, 1.0000e+00])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# any fixed offset does not change the output, so pytorch internally does something like this,\n",
    "# subtracting the highest input value from the entire vector to avoid any overflow.\n",
    "logits = torch.tensor([-100, -3, 0, 100]) - 100\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum()\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e242665b-9908-490e-9f91-6c7f933df5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c8f9f6d9-2dfa-44f1-ac7f-546b3ca18c5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 loss=10.709585189819336\n",
      " 1 loss=10.40762996673584\n",
      " 2 loss=10.127808570861816\n",
      " 3 loss=9.864365577697754\n",
      " 4 loss=9.61450481414795\n",
      " 5 loss=9.37644100189209\n",
      " 6 loss=9.148946762084961\n",
      " 7 loss=8.931112289428711\n",
      " 8 loss=8.722232818603516\n",
      " 9 loss=8.52175235748291\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    # forward pass\n",
    "    emb = C[X] # 32x2x2\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # -1 means torch will infer the size - output is 32x100\n",
    "    logits = h @ W2 + b2 # 32x27\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    print(f'{i:2d} loss={loss.item()}')\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    for p in parameters:\n",
    "        p.data += -0.1 * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "5e7ce459-7f19-4d0e-b4db-9dc042a2171d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999 loss=2.192488431930542\n"
     ]
    }
   ],
   "source": [
    "# each pass taking a while, lets do mini batches now\n",
    "for i in range(10000):\n",
    "\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, X.shape[0], (32,))\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[X[ix]] # 32x3x2\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # -1 means torch will infer the size - output is 32x100\n",
    "    logits = h @ W2 + b2 # 32x27\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    for p in parameters:\n",
    "        p.data += -.01 * p.grad\n",
    "print(f'{i:2d} loss={loss.item()}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cfc8a738-98a1-44c3-83c1-8f381921660a",
   "metadata": {},
   "source": [
    "Mini batches make it so our gradient is lower quality, we sometimes take a step away from minimizing the loss. However, the training loop runs *much* faster, so it is still better to do mini-batches and just take many more steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "1f64bd82-0971-4faf-965c-82ddfad3471f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3079402446746826"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate loss for entire dataset\n",
    "emb = C[X]\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691d325a-6c47-4a53-a778-eacb59c92e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
